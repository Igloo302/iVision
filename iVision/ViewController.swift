/*
 See LICENSE folder for this sampleâ€™s licensing information.
 
 Abstract:
 Contains the view controller for the Breakfast Finder.
 */

import UIKit
import AVFoundation
import Vision

import SceneKit
import ARKit

class ViewController: UIViewController, AVCaptureVideoDataOutputSampleBufferDelegate,ARSCNViewDelegate,ARSessionDelegate {
    
    let showLayer = false
    
    public static let inputWidth = 416
    public static let inputHeight = 416
    public static let maxBoundingBoxes = 10
    
    // Tweak these values to get more or fewer predictions.
    let confidenceThreshold: Float = 0.3
    let iouThreshold: Float = 0.5
    
    // è¯†åˆ«çš„ç‰©ä½“
    struct Prediction {
        let classIndex: Int
        let score: Float
        let rect: CGRect
        let worldCoord: [SCNVector3]
    }
    
    var predictions = [Prediction]()
    var nodes = [SCNNode]()
    
    // è¯†åˆ«åˆ°çš„ç‰©ä½“çš„åå­—
    var labels = [String]()
    
    //è®¡æ—¶å™¨
    var startTimes: [CFTimeInterval] = []
    
    // æ˜¾ç¤ºè¯†åˆ«ç»“æœçš„Boundingçš„å›¾å±‚
    var detectionOverlay: CALayer! = nil
    var rootLayer: CALayer! = nil
    
    // å±å¹•å°ºå¯¸
    var bufferSize: CGSize = .zero
    
    // COREML
    var visionRequests = [VNRequest]()
    
    @IBOutlet weak var debugTextView: UITextView!
    
    @IBOutlet weak var sceneView: ARSCNView!
    
    let bubbleDepth : Float = 0.01 // the 'depth' of 3D text æ–‡å­—çš„åšåº¦
    
    // variable containing the latest CoreML prediction & position
    var latestPrediction : String = ""
    var latestPredictionPosition: CGPoint = .zero
    
    override func viewDidLoad() {
        super.viewDidLoad()
        
        // æ‰§è¡Œ UIView
        // setupAVCapture()
        
        // Set the view's delegate
        sceneView.delegate = self
        sceneView.session.delegate = self
        
        // Show statistics such as fps and timing information
        sceneView.showsStatistics = true
        
        sceneView.preferredFramesPerSecond = 30
        
        // Create a new scene
        let scene = SCNScene()
        
        // Set the scene to the view
        sceneView.scene = scene
        
        // Enable Default Lighting - makes the 3D text a bit poppier.
        sceneView.autoenablesDefaultLighting = true
        
        // sceneView.debugOptions = [.showFeaturePoints]
        
        bufferSize = CGSize(width: sceneView.bounds.height, height: sceneView.bounds.width)
        print("ğŸ˜æŠŠBufferSizeè®¾ç½®æˆ", bufferSize)
        
        //é…ç½®Layeråˆå§‹åŒ–
        rootLayer = sceneView.layer
        setupLayers()
        updateLayerGeometry()
        
        // Tap Gesture Recognizer ç‚¹å‡»æ“ä½œè¯†åˆ«å™¨
        let tapGesture = UITapGestureRecognizer(target: self, action: #selector(self.handleTap(gestureRecognize:)))
        view.addGestureRecognizer(tapGesture)
        
        // è®¾ç½®YOLOè¯†åˆ«å™¨
        // Vision classification request and model
        
        guard let ARmodelURL = Bundle.main.url(forResource: "YOLOv3Tiny", withExtension: "mlmodelc") else {fatalError("æ²¡æœ‰æ‰¾åˆ°YOLOv3æ¨¡å‹ï¼Œå‡‰å‡‰")
        }
        do {
            // è½½å…¥æ¨¡å‹
            let visionModel = try VNCoreMLModel(for: MLModel(contentsOf: ARmodelURL))
            print("ğŸ˜SceneKitæ‰€éœ€æ¨¡å‹è½½å…¥æˆåŠŸ")
            // ä½¿ç”¨è¯¥æ¨¡å‹åˆ›å»ºä¸€ä¸ªVNCoreMLRequestï¼Œè¯†åˆ«åˆ°ä¹‹åæ‰§è¡ŒcompletionHandleré‡Œé¢çš„éƒ¨åˆ†
            let ARobjectRecognition = VNCoreMLRequest(model: visionModel, completionHandler: objectRecognitionCompleteHandler)
            
            // Crop input images to square area at center, matching the way the ML model was trained.
            
            // NOTE: If you choose another crop/scale option, then you must also
            // change how the BoundingBox objects get scaled when they are drawn.
            // Currently they assume the full input image is used.
            ARobjectRecognition.imageCropAndScaleOption = .scaleFill
            
            // Use CPU for Vision processing to ensure that there are adequate GPU resources for rendering.
            // ARobjectRecognition.usesCPUOnly = true
            
            visionRequests = [ARobjectRecognition]
        } catch let error as NSError {
            print("Model loading went wrong: \(error)")
        }
        
    }
    
    
    override func viewWillAppear(_ animated: Bool) {
        super.viewWillAppear(animated)
        
        // Create a session configuration
        let configuration = ARWorldTrackingConfiguration()
        
        // Enable plane detection
        // configuration.planeDetection = [.horizontal, .vertical]
        
        // Run the view's session
        sceneView.session.run(configuration)
        print("ğŸ˜AR Configurationè½½å…¥æˆåŠŸ")
        
    }
    
    override func viewWillDisappear(_ animated: Bool) {
        super.viewWillDisappear(animated)
        
        // Pause the view's session
        sceneView.session.pause()
    }
    
    // MARK: - ARSessionDelegate
    
    // Pass camera frames received from ARKit to Vision (when not already processing one)
    /// - Tag: ConsumeARFrames
    func session(_ session: ARSession, didUpdate frame: ARFrame) {
        // Do not enqueue other buffers for processing while another Vision task is still running.
        // The camera stream has only a finite amount of buffers available; holding too many buffers for analysis would starve the camera.
        guard pixbuff == nil, case .normal = frame.camera.trackingState else {
            return
        }
        
        // Retain the image buffer for Vision processing.
        self.pixbuff = frame.capturedImage
        updateCoreML()
    }
    
    override func didReceiveMemoryWarning() {
        super.didReceiveMemoryWarning()
        // Dispose of any resources that can be recreated.
    }
    
    
    // MARK: - Status Bar: Hide
    override var prefersStatusBarHidden : Bool {
        return true
    }
    
    // MARK: - Interaction
    
    
    @objc func handleTap(gestureRecognize: UITapGestureRecognizer) {
        
        // çœŸæ­£çš„ä½ç½®ï¼
        // print("ğŸŒšlatestPredictionPosition", latestPredictionPosition)
        let HitTestResults : [ARHitTestResult] = sceneView.hitTest(latestPredictionPosition, types: [.featurePoint])
        
        if let closestResult = HitTestResults.first {
            // Get Coordinates of HitTest
            let transform : matrix_float4x4 = closestResult.worldTransform
            let worldCoord : SCNVector3 = SCNVector3Make(transform.columns.3.x, transform.columns.3.y, transform.columns.3.z)
            
            // Create 3D Text
            let node : SCNNode = createNewBubbleParentNode(latestPrediction)
            sceneView.scene.rootNode.addChildNode(node)
            node.position = worldCoord
            node.name = latestPrediction
            print(worldCoord)
        }
    }
    
    
    func showNode() {
        // çœŸæ­£çš„ä½ç½®ï¼
        // print("ğŸŒšlatestPredictionPosition", latestPredictionPosition)
        let HitTestResults : [ARHitTestResult] = sceneView.hitTest(latestPredictionPosition, types: [.featurePoint])
        
        if let closestResult = HitTestResults.first {
            // Get Coordinates of HitTest
            let transform : matrix_float4x4 = closestResult.worldTransform
            let worldCoord : SCNVector3 = SCNVector3Make(transform.columns.3.x, transform.columns.3.y, transform.columns.3.z)
            
            // Create 3D Text
            let node : SCNNode = createNewBubbleParentNode(latestPrediction)
            sceneView.scene.rootNode.addChildNode(node)
            node.position = worldCoord
        }
    }
    
    
    
    func createNewBubbleParentNode(_ text : String) -> SCNNode {
        // Warning: Creating 3D Text is susceptible to crashing. To reduce chances of crashing; reduce number of polygons, letters, smoothness, etc.
        let bubbleNodeParent = SCNNode()
        bubbleNodeParent.addChildNode(createBubbleNode(bubble: text))
        bubbleNodeParent.addChildNode(createSphereNode())
        bubbleNodeParent.constraints = [createBillboardConstraint()]
        return bubbleNodeParent
    }
    
    private func createBubbleNode(bubble: String) -> SCNNode {
        let bubble = createBubble(text: bubble)
        //        // è®©Siriè¯´è¯´è¯
        //        var stringToSpeak = String("")
        //        for _ in 0..<100 {
        //            stringToSpeak += (bubble + "is here.")
        //        }
        //
        //        let utterance = AVSpeechUtterance(string: stringToSpeak)
        //        let synthesizer = AVSpeechSynthesizer()
        //        synthesizer.speak(utterance)
        //
        //        let avMaterial = SCNMaterial()
        //        avMaterial.diffuse.contents = synthesizer
        //        bubble.materials = [avMaterial]
        let (minBound, maxBound) = bubble.boundingBox
        let bubbleNode = SCNNode(geometry: bubble)
        // Centre Node - to Centre-Bottom point
        bubbleNode.pivot = SCNMatrix4MakeTranslation( (maxBound.x - minBound.x)/2, minBound.y, bubbleDepth/2)
        // Reduce default text size
        bubbleNode.scale = SCNVector3Make(0.2, 0.2, 0.2)
        
        //        //æ’­æ”¾éŸ³ä¹
        //        let audioSource = SCNAudioSource(fileNamed: "Dog.mp3")!
        //        // As an environmental sound layer, audio should play indefinitely
        //        audioSource.loops = true
        //        // Decode the audio from disk ahead of time to prevent a delay in playback
        //        audioSource.load()
        //
        //        // Create a player from the source and add it to `objectNode`
        //        bubbleNode.addAudioPlayer(SCNAudioPlayer(source: audioSource))
        
        return bubbleNode
    }
    
    private func createBubble(text: String) -> SCNText {
        let bubble = SCNText(string: text, extrusionDepth: CGFloat(bubbleDepth))
        var font = UIFont(name: "Futura", size: 0.15)
        font = font?.withTraits(traits: .traitBold)
        bubble.font = font
        bubble.alignmentMode = convertFromCATextLayerAlignmentMode(CATextLayerAlignmentMode.center)
        bubble.firstMaterial?.diffuse.contents = UIColor.orange
        bubble.firstMaterial?.specular.contents = UIColor.white
        bubble.firstMaterial?.isDoubleSided = true
        // bubble.flatness // setting this too low can cause crashes.
        bubble.chamferRadius = CGFloat(bubbleDepth)
        return bubble
    }
    
    private func createSphereNode() -> SCNNode {
        let sphere = SCNSphere(radius: 0.005)
        sphere.firstMaterial?.diffuse.contents = UIColor.cyan
        return SCNNode(geometry: sphere)
    }
    
    private func createBillboardConstraint() -> SCNBillboardConstraint {
        let billboardConstraint = SCNBillboardConstraint()
        billboardConstraint.freeAxes = SCNBillboardAxis.Y
        return billboardConstraint
    }
    
    
    // MARK: - CoreML Vision Handling
    
    private let visionQueue = DispatchQueue(label: "com.example.apple-samplecode.ARKitVision.serialVisionQueue")
    // The pixel buffer being held for analysis; used to serialize Vision requests.
    private var pixbuff: CVPixelBuffer?
    
    // âœŒï¸é‡è¦ï¼æŠŠç”»é¢å¼„åˆ°core MLæ¨¡å‹è¿›è¡Œè¯†åˆ«çš„éƒ¨åˆ†ï¼
    func updateCoreML() {
        // Get Camera Image as RGB SCENEVIEWçš„å›¾ç‰‡å°±ç”¨å½“å‰å¸§çš„ç”»é¢å¼„å‡ºæ¥å°±è¡Œ
        pixbuff  = sceneView.session.currentFrame?.capturedImage
        if pixbuff == nil { return }
        
        // è®¡æ—¶å™¨
        startTimes.append(CACurrentMediaTime())
        
        // Prepare CoreML/Vision Requestï¼ŒVNImageRequestHandleræ˜¯å¤„ç†ä¸å•ä¸ªå›¾åƒæœ‰å…³çš„ä¸€ä¸ªæˆ–å¤šä¸ªå›¾åƒåˆ†æè¯·æ±‚çš„å¯¹è±¡
        // Vision will automatically resize the input image.
        let imageRequestHandler = VNImageRequestHandler(cvPixelBuffer: pixbuff!, options: [:])
        
        // Run Image Request
        visionQueue.async {
            do {
                // Release the pixel buffer when done, allowing the next buffer to be processed.
                defer { self.pixbuff = nil }
                // è¯†åˆ«å™¨
                try imageRequestHandler.perform(self.visionRequests)
            } catch {
                print("Error: Vision request failed with error \"\(error)\"")
            }
        }
    }
    
    
    // è¯†åˆ«å™¨å®Œæˆä¹‹åå¹²çš„äº‹æƒ…
    // Handle completion of the Vision request and choose results to display.
    func objectRecognitionCompleteHandler(request: VNRequest, error: Error?){
        // Catch Errors
        guard let observations = request.results else {
            print("Unable to classify image.\n\(error!.localizedDescription)")
            return
        }
        
        // è¾“å‡ºæ¨¡å‹å»¶è¿Ÿ
        let elapsed = CACurrentMediaTime() - startTimes.remove(at: 0)
        print(String(format: "Elapsed %.5f seconds", elapsed))
        
        showOnMainThread(observations, elapsed)
    }
    
    // MARK: - Something New
    
    func showOnMainThread(_ observations: [Any],_ elapsed: CFTimeInterval){
        //åˆ é™¤ä¹‹å‰çš„sublayers
        detectionOverlay.sublayers = nil
        
        // Get ObjectRecognition
        for observation in observations where observation is VNRecognizedObjectObservation{
            guard let objectObservation = observation as? VNRecognizedObjectObservation else {
                continue
            }
            // Select only the label with the highest confidence.
            // æ ‡ç­¾æ•°ç»„åˆ—å‡ºæ¯ä¸ªåˆ†ç±»æ ‡è¯†ç¬¦åŠå…¶ç½®ä¿¡åº¦å€¼ï¼Œä»æœ€é«˜ç½®ä¿¡åº¦åˆ°æœ€ä½ç½®ä¿¡åº¦æ’åºã€‚
            // è¯¥ç¤ºä¾‹åº”ç”¨ç¨‹åºä»…åœ¨å…ƒç´ 0å¤„è®°å½•äº†å…·æœ‰æœ€é«˜ç½®ä¿¡åº¦å¾—åˆ†çš„åˆ†ç±»ã€‚
            // ç„¶åï¼Œå®ƒä¼šåœ¨æ–‡æœ¬å åŠ å±‚ä¸­æ˜¾ç¤ºæ­¤åˆ†ç±»å’Œç½®ä¿¡åº¦ã€‚
            let topLabelObservation = objectObservation.labels[0]
            // å–ç½®ä¿¡åº¦é«˜çš„
            if topLabelObservation.confidence > confidenceThreshold {
                
                let objectBounds = VNImageRectForNormalizedRect(objectObservation.boundingBox, Int(bufferSize.width), Int(bufferSize.height))
                
                addToPredictions(objectBounds, identifier: topLabelObservation.identifier, confidence: topLabelObservation.confidence)
                
                // updateNode
                updateNode(predictions.last!)
                
                // ä¿å­˜è¯†åˆ«çš„ç»“æœ
                self.latestPrediction = topLabelObservation.identifier
                self.latestPredictionPosition = CGPoint(x:objectBounds.midY, y: objectBounds.midX)
                
                DispatchQueue.main.async {
                    self.updateUI(topLabelObservation)
                    
                    if self.showLayer {
                        self.updateLayer(topLabelObservation, objectBounds)
                    }
                }
                
            }
        }
        
        
    }
    
    func updateNode(_ prediction: Prediction){
//        print(prediction.classIndex)
//        print(labels.count)
//        print(labels[prediction.classIndex])
        
        var nodeExsit = false
        for node in nodes {
            if node.name == labels[prediction.classIndex]{
                // print("Change Position")
                node.position = prediction.worldCoord.first!
                nodeExsit = true
                continue
            }
        }
        if !nodeExsit{
            // print("Add New Node")
            let node = createNewBubbleParentNode(labels[prediction.classIndex])
            node.position = prediction.worldCoord.first!
            node.name = labels[prediction.classIndex]
            nodes.append(node)
            sceneView.scene.rootNode.addChildNode(node)
        }
    }
    
    func updateUI(_ topLabelObservation: VNClassificationObservation){
        var debugText:String = ""
        debugText += "\(topLabelObservation.identifier)" + "\(topLabelObservation.confidence)"
        self.debugTextView.text = debugText
    }
    //æ˜¾ç¤ºLayer
    func updateLayer(_ topLabelObservation: VNClassificationObservation, _ objectBounds: CGRect){
        
        
        let shapeLayer = self.createRoundedRectLayerWithBounds(objectBounds, identifier: topLabelObservation.identifier)
        let textLayer = self.createTextSubLayerInBounds(objectBounds, identifier: topLabelObservation.identifier, confidence: topLabelObservation.confidence)
        // CATransaction.setDisableActions(true)
        shapeLayer.addSublayer(textLayer)
        self.detectionOverlay.addSublayer(shapeLayer)
        
    }
    
    
    func addToPredictions(_ rect: CGRect, identifier: String, confidence: VNConfidence){
        
        let worldCoord = [getWorldCoord(rect)]
        
        let prediction = Prediction(classIndex: getClassIndex(identifier: identifier), score: confidence * 100, rect: rect, worldCoord: worldCoord)
        
        // print(prediction)
        
        predictions.append(prediction)
    }
    
    func getClassIndex(identifier: String) -> Int{
        if let indexOf = labels.firstIndex(of: identifier) {
            return indexOf
        }else{
            labels.append(identifier)
            return labels.firstIndex(of: identifier)!
        }
    }
    
    func getWorldCoord(_ rect: CGRect) -> SCNVector3{
        
        let screenPoint = CGPoint(x:rect.midY, y: rect.midX)
        let HitTestResults : [ARHitTestResult] = sceneView.hitTest(screenPoint, types: [.featurePoint])
        
        if let closestResult = HitTestResults.first {
            // Get Coordinates of HitTest
            let transform : matrix_float4x4 = closestResult.worldTransform
            let worldCoord : SCNVector3 = SCNVector3Make(transform.columns.3.x, transform.columns.3.y, transform.columns.3.z)
            return worldCoord
        }else{return SCNVector3(0, 0, 0)}
    }
    
    func setupLayers() {
        // container layer that has all the renderings of the observations
        detectionOverlay = CALayer()
        
        detectionOverlay.name = "DetectionOverlay"
        detectionOverlay.bounds = CGRect(x: 0.0,
                                         y: 0.0,
                                         width: bufferSize.width,
                                         height: bufferSize.height)
        detectionOverlay.position = CGPoint(x: rootLayer.bounds.midX, y: rootLayer.bounds.midY)
        rootLayer.addSublayer(detectionOverlay)
    }
    
    func updateLayerGeometry() {
        let bounds = rootLayer.bounds
        var scale: CGFloat
        
        let xScale: CGFloat = bounds.size.width / bufferSize.height
        let yScale: CGFloat = bounds.size.height / bufferSize.width
        
        scale = fmax(xScale, yScale)
        if scale.isInfinite {
            scale = 1.0
        }
        //        CATransaction.begin()
        //        CATransaction.setValue(kCFBooleanTrue, forKey: kCATransactionDisableActions)
        
        // rotate the layer into screen orientation and scale and mirror
        detectionOverlay.setAffineTransform(CGAffineTransform(rotationAngle: CGFloat(.pi / 2.0)).scaledBy(x: scale, y: -scale))
        // center the layer
        detectionOverlay.position = CGPoint (x: bounds.midX, y: bounds.midY)
        
        //        CATransaction.commit()
        
    }
    
    func createTextSubLayerInBounds(_ bounds: CGRect, identifier: String, confidence: VNConfidence) -> CATextLayer {
        let textLayer = CATextLayer()
        textLayer.name = "Object Label"
        let formattedString = NSMutableAttributedString(string: String(format: "\(identifier)\n%.2f", confidence))
        let largeFont = UIFont(name: "Helvetica", size: 12.0)!
        formattedString.addAttributes([NSAttributedString.Key.font: largeFont], range: NSRange(location: 0, length: identifier.count))
        textLayer.string = formattedString
        textLayer.bounds = CGRect(x: 0, y: 0, width: bounds.size.height - 10, height: bounds.size.width - 10)
        textLayer.position = CGPoint(x: bounds.midX, y: bounds.midY)
        textLayer.shadowOpacity = 0.7
        textLayer.shadowOffset = CGSize(width: 2, height: 2)
        textLayer.foregroundColor = CGColor(colorSpace: CGColorSpaceCreateDeviceRGB(), components: [0.0, 0.0, 0.0, 1.0])
        textLayer.contentsScale = 2.0 // retina rendering
        // rotate the layer into screen orientation and scale and mirror
        textLayer.setAffineTransform(CGAffineTransform(rotationAngle: CGFloat(.pi / 2.0)).scaledBy(x: 1.0, y: -1.0))
        return textLayer
    }
    
    func createRoundedRectLayerWithBounds(_ bounds: CGRect, identifier: String) -> CALayer {
        let shapeLayer = CALayer()
        shapeLayer.bounds = bounds
        shapeLayer.position = CGPoint(x: bounds.midX, y: bounds.midY)
        shapeLayer.name = identifier
        shapeLayer.backgroundColor = CGColor(colorSpace: CGColorSpaceCreateDeviceRGB(), components: [1.0, 1.0, 0.2, 0.4])
        shapeLayer.cornerRadius = 7
        return shapeLayer
    }
    
}

extension UIFont {
    // Based on: https://stackoverflow.com/questions/4713236/how-do-i-set-bold-and-italic-on-uilabel-of-iphone-ipad
    func withTraits(traits:UIFontDescriptor.SymbolicTraits...) -> UIFont {
        let descriptor = self.fontDescriptor.withSymbolicTraits(UIFontDescriptor.SymbolicTraits(traits))
        return UIFont(descriptor: descriptor!, size: 0)
    }
}

// Helper function inserted by Swift 4.2 migrator.
fileprivate func convertFromCATextLayerAlignmentMode(_ input: CATextLayerAlignmentMode) -> String {
    return input.rawValue
}
